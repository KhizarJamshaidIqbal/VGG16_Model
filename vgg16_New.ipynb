{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhizarJamshaidIqbal/VGG16_Model/blob/main/vgg16_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import All NecessaryPackages**"
      ],
      "metadata": {
        "id": "DOhy6ynQmGys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from PIL import Image\n",
        "from tensorflow import lite\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "lilOUC5sl8g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Connect With the GoogleDrive**"
      ],
      "metadata": {
        "id": "j9NKh2Jyme_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db4--PiJmeHM",
        "outputId": "59d9554c-2d1b-41e1-ef2a-d3f47e1dd2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing the Data"
      ],
      "metadata": {
        "id": "Pmbznb6kms-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# re-size all the images to this\n",
        "IMAGE_SIZE = [224, 224]\n",
        "\n",
        "\n",
        "train_path = '/content/gdrive/MyDrive/dataset/Pakistan/Training'\n",
        "valid_path = '/content/gdrive/MyDrive/dataset/Pakistan/Valid'\n",
        "\n",
        "# add preprocessing layer to the front of VGG\n",
        "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "# don't train existing weights\n",
        "for layer in vgg.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# useful for getting number of classes\n",
        "folders = glob('/content/gdrive/MyDrive/dataset/Pakistan/Training/*')\n",
        "print(folders)\n",
        "\n",
        "# our layers - you can add more if you want\n",
        "x = Flatten()(vgg.output)\n",
        "# x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "\n",
        "# create a model object\n",
        "model = Model(inputs=vgg.input, outputs=prediction)\n",
        "\n",
        "# view the structure of the model\n",
        "model.summary()\n",
        "\n",
        "# tell the model what cost and optimization method to use\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0faYIGszmFar",
        "outputId": "73e14b0d-4758-4b6e-cce0-6fec26c684cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "['/content/gdrive/MyDrive/dataset/Pakistan/Training/50Rs', '/content/gdrive/MyDrive/dataset/Pakistan/Training/5000Rs', '/content/gdrive/MyDrive/dataset/Pakistan/Training/500Rs', '/content/gdrive/MyDrive/dataset/Pakistan/Training/50Rsback', '/content/gdrive/MyDrive/dataset/Pakistan/Training/500Rsback', '/content/gdrive/MyDrive/dataset/Pakistan/Training/5000Rsback', '/content/gdrive/MyDrive/dataset/Pakistan/Training/10Rsback', '/content/gdrive/MyDrive/dataset/Pakistan/Training/10Rs', '/content/gdrive/MyDrive/dataset/Pakistan/Training/20Rs', '/content/gdrive/MyDrive/dataset/Pakistan/Training/100Rsback', '/content/gdrive/MyDrive/dataset/Pakistan/Training/20Rsback', '/content/gdrive/MyDrive/dataset/Pakistan/Training/100Rs', '/content/gdrive/MyDrive/dataset/Pakistan/Training/1000Rs', '/content/gdrive/MyDrive/dataset/Pakistan/Training/1000Rsback', '/content/gdrive/MyDrive/dataset/Pakistan/Training/Other']\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 15)                376335    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15091023 (57.57 MB)\n",
            "Trainable params: 376335 (1.44 MB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ImageDataGenerator**"
      ],
      "metadata": {
        "id": "bm1knasBoI4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/gdrive/MyDrive/dataset/Pakistan/Training',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/gdrive/MyDrive/dataset/Pakistan/Valid',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPwMWngjoFho",
        "outputId": "5971c235-9d0d-4edf-9eec-8966df47472b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2066 images belonging to 15 classes.\n",
            "Found 241 images belonging to 15 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run epochs**"
      ],
      "metadata": {
        "id": "j21yHTfApaM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "r = model.fit_generator(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=15,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QiPInt1odA8",
        "outputId": "e0fc18d3-04cf-43af-8338-0da15c077995"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-051908e78a64>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  r = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "51/65 [======================>.......] - ETA: 2:57 - loss: 2.4074 - accuracy: 0.3412"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **loss Graph**"
      ],
      "metadata": {
        "id": "7UWfIwW6o7g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')"
      ],
      "metadata": {
        "id": "ud8bPESyo58F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracies\n",
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')  # Corrected key\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ],
      "metadata": {
        "id": "hJ1w8sm4o_x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saved Model**"
      ],
      "metadata": {
        "id": "39ICCctHpMii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save('/content/gdrive/MyDrive/Models/VGG16/VGG16_AC_69/VGG16.h5')\n",
        "model_save_path = '/content/gdrive/MyDrive/Models/VGG16.h5'\n",
        "model.save(model_save_path)\n",
        "print('Your Model is==> ',os.getcwd())\n",
        "# Confirm the save path\n",
        "print(f\"Model saved successfully at: {model_save_path}\")"
      ],
      "metadata": {
        "id": "HmeNYTkSpRN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing The Model**"
      ],
      "metadata": {
        "id": "7Uy--ck71p4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model_path = model_save_path\n",
        "#'Sa_VGG19_digitRecognizer_Model_h5'\n",
        "# Update with the actual path to your saved model\n",
        "loaded_model = load_model(model_path)"
      ],
      "metadata": {
        "id": "D0YkKFJ21tye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class names\n",
        "class_names = sorted(training_set.class_indices.keys())\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "_-ucgT9814Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the test image\n",
        "test_img_path = '/content/download.jpg'\n",
        "\n",
        "# Function to preprocess an image\n",
        "def preprocess_img(img_path):\n",
        "    img = Image.open(img_path)  # Use PIL to open the image\n",
        "    img = img.resize((224, 224))  # Resize the image\n",
        "    img_array = np.array(img)  # Convert PIL image to NumPy array\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Normalize pixel values\n",
        "    return img_array\n",
        "\n",
        "# Function to predict the class of an image\n",
        "def predict_class(img_path):\n",
        "    img = preprocess_img(img_path)\n",
        "    prediction = loaded_model.predict(img)\n",
        "    predicted_class = class_names[np.argmax(prediction)]\n",
        "    return predicted_class\n",
        "\n",
        "# Predict the class for the test image\n",
        "predicted_class = predict_class(test_img_path)\n",
        "\n",
        "# Display the test image and predicted class\n",
        "test_img = cv2.imread(test_img_path)\n",
        "test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(test_img)\n",
        "plt.title(f'Predicted Class: {predicted_class}')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pl07bB7J2SIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saved**\n"
      ],
      "metadata": {
        "id": "Hd3AVMPW3_js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TFLite Tensorflow Model**"
      ],
      "metadata": {
        "id": "v-tBaRC_4x7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Keras model\n",
        "model = load_model(model_save_path)\n",
        "\n",
        "# Convert the model to TensorFlow Lite format\n",
        "converter = lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model\n",
        "with open('/content/gdrive/MyDrive/Models/VGG16_Model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "euynB4QA4Am4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**labels.txt**"
      ],
      "metadata": {
        "id": "bCBZdrtS4bg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace this list with your actual class labels\n",
        "class_labels = class_names\n",
        " #['1000Rs', '1000Rsback', '100Rs', '100Rsback', '10Rs', '10Rsback', '20Rs', '20Rsback', '5000Rs', '5000Rsback', '500Rs', '500Rsback', '50Rs', '50Rsback']\n",
        "\n",
        "# Save the class labels to a file\n",
        "with open(\"labels.txt\", \"w\") as file:\n",
        "    for label in class_labels:\n",
        "        file.write(label + \"\\n\")\n",
        "\n",
        "# Move the file to the desired directory (e.g., /content/gdrive/MyDrive/PCSD_App/)\n",
        "import shutil\n",
        "shutil.move(\"labels.txt\", \"/content/gdrive/MyDrive/Models/\")\n"
      ],
      "metadata": {
        "id": "N0CXRhNz4cR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}